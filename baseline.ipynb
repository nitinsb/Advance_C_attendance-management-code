{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nitinsb/Advance_C_attendance-management-code/blob/master/baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kCkfRFVDVbp"
      },
      "outputs": [],
      "source": [
        "# baseline : permute,-> context 1\n",
        "# Standard libraries\n",
        "import numpy as np\n",
        "import copy\n",
        "import tqdm\n",
        "# Pytorch\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "# For visualization\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MNIST_trainset = datasets.MNIST(root='data/', train=True, download=True,\n",
        "                                transform=transforms.ToTensor())\n",
        "MNIST_testset = datasets.MNIST(root='data/', train=False, download=True,\n",
        "                               transform=transforms.ToTensor())\n",
        "config = {'size': 28, 'channels': 1, 'classes': 10}\n"
      ],
      "metadata": {
        "id": "2qSWzuFaFeC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualization functions\n",
        "def multi_context_barplot(axis, accs, title=None):\n",
        "    '''Generate barplot using the values in [accs].'''\n",
        "    contexts = len(accs)\n",
        "    axis.bar(range(contexts), accs, color='k')\n",
        "    axis.set_ylabel('Testing Accuracy (%)')\n",
        "    axis.set_xticks(range(contexts), [f'Context {i+1}' for i in range(contexts)])\n",
        "    if title is not None:\n",
        "        axis.set_title(title)\n",
        "\n",
        "def plot_examples(axis, dataset, context_id=None):\n",
        "    '''Plot 25 examples from [dataset].'''\n",
        "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=25, shuffle=True)\n",
        "    image_tensor, _ = next(iter(data_loader))\n",
        "    image_grid = make_grid(image_tensor, nrow=5, pad_value=1) # pad_value=0 would give black borders\n",
        "    axis.imshow(np.transpose(image_grid.numpy(), (1,2,0)))\n",
        "    if context_id is not None:\n",
        "        axis.set_title(\"Context {}\".format(context_id+1))\n",
        "    axis.axis('off')\n"
      ],
      "metadata": {
        "id": "vDNVgNuPFmxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to apply a given permutation the pixels of an image.\n",
        "def permutate_image_pixels(image, permutation):\n",
        "    '''Permutate the pixels of [image] according to [permutation].'''\n",
        "\n",
        "    if permutation is None:\n",
        "        return image\n",
        "    else:\n",
        "        c, h, w = image.size()\n",
        "        image = image.view(c, -1)\n",
        "        image = image[:, permutation]  #--> same permutation for each channel\n",
        "        image = image.view(c, h, w)\n",
        "        return image"
      ],
      "metadata": {
        "id": "6k9zRwvYFrOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Class to create a dataset with images that have all been transformed in the same way.\n",
        "class TransformedDataset(torch.utils.data.Dataset):\n",
        "    '''To modify an existing dataset with a transform.\n",
        "    Useful for creating different permutations of MNIST without loading the data multiple times.'''\n",
        "\n",
        "    def __init__(self, original_dataset, transform=None, target_transform=None):\n",
        "        super().__init__()\n",
        "        self.dataset = original_dataset\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        (input, target) = self.dataset[index]\n",
        "        if self.transform:\n",
        "            input = self.transform(input)\n",
        "        if self.target_transform:\n",
        "            target = self.target_transform(target)\n",
        "        return (input, target)\n"
      ],
      "metadata": {
        "id": "L1Y3FFkqFwHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contexts = 2"
      ],
      "metadata": {
        "id": "x8OPCyhLF0xW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify for each context the permutations to use (with no permutation for the first context)\n",
        "permutations = [None] + [np.random.permutation(config['size']**2) for _ in range(contexts-1)]"
      ],
      "metadata": {
        "id": "xS32eRRGF3LT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify for each context the transformed train- and testset\n",
        "train_datasets = []\n",
        "test_datasets = []\n",
        "for context_id, perm in enumerate(permutations):\n",
        "    train_datasets.append(TransformedDataset(\n",
        "        MNIST_trainset, transform=transforms.Lambda(lambda x, p=perm: permutate_image_pixels(x, p)),\n",
        "    ))\n",
        "    test_datasets.append(TransformedDataset(\n",
        "        MNIST_testset, transform=transforms.Lambda(lambda x, p=perm: permutate_image_pixels(x, p)),\n",
        "    ))\n"
      ],
      "metadata": {
        "id": "W9twIwUJF6DK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the contexts\n",
        "figure, axis = plt.subplots(1, contexts, figsize=(3*contexts, 4))\n",
        "\n",
        "for context_id in range(len(train_datasets)):\n",
        "    plot_examples(axis[context_id], train_datasets[context_id], context_id=context_id)"
      ],
      "metadata": {
        "id": "Kz1lxHElF_Bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper functions\n",
        "\n",
        "class Identity(torch.nn.Module):\n",
        "    '''A nn-module to simply pass on the input data.'''\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "    def __repr__(self):\n",
        "        tmpstr = self.__class__.__name__ + '()'\n",
        "        return tmpstr\n",
        "\n",
        "\n",
        "class Flatten(torch.nn.Module):\n",
        "    '''A nn-module to flatten a multi-dimensional tensor to 2-dim tensor.'''\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)   # first dimenstion should be batch-dimension.\n",
        "        return x.view(batch_size, -1)\n",
        "\n",
        "    def __repr__(self):\n",
        "        tmpstr = self.__class__.__name__ + '()'\n",
        "        return tmpstr\n",
        "\n",
        "\n",
        "class fc_layer(torch.nn.Module):\n",
        "    '''Fully connected layer, with possibility of returning \"pre-activations\".\n",
        "\n",
        "    Input:  [batch_size] x ... x [in_size] tensor\n",
        "    Output: [batch_size] x ... x [out_size] tensor'''\n",
        "\n",
        "    def __init__(self, in_size, out_size, nl=torch.nn.ReLU(), bias=True):\n",
        "        super().__init__()\n",
        "        self.bias = bias\n",
        "        self.linear = torch.nn.Linear(in_size, out_size, bias=bias)\n",
        "        if isinstance(nl, torch.nn.Module):\n",
        "            self.nl = nl\n",
        "        elif nl==\"relu\":\n",
        "            self.nl = torch.nn.ReLU()\n",
        "        elif nl==\"leakyrelu\":\n",
        "          self.nl = torch.nn.LeakyReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        pre_activ = self.linear(x)\n",
        "        output = self.nl(pre_activ) if hasattr(self, 'nl') else pre_activ\n",
        "        return output\n",
        "\n",
        "\n",
        "class MLP(torch.nn.Module):\n",
        "    '''Module for a multi-layer perceptron (MLP).\n",
        "\n",
        "    Input:  [batch_size] x ... x [size_per_layer[0]] tensor\n",
        "    Output: (tuple of) [batch_size] x ... x [size_per_layer[-1]] tensor'''\n",
        "\n",
        "    def __init__(self, input_size=1000, output_size=10, layers=2,\n",
        "                 hid_size=1000, hid_smooth=None, size_per_layer=None,\n",
        "                 nl=\"relu\", bias=True, output='normal'):\n",
        "        '''sizes: 0th=[input], 1st=[hid_size], ..., 1st-to-last=[hid_smooth], last=[output].\n",
        "        [input_size]       # of inputs\n",
        "        [output_size]      # of units in final layer\n",
        "        [layers]           # of layers\n",
        "        [hid_size]         # of units in each hidden layer\n",
        "        [hid_smooth]       if None, all hidden layers have [hid_size] units, else # of units linearly in-/decreases s.t.\n",
        "                             final hidden layer has [hid_smooth] units (if only 1 hidden layer, it has [hid_size] units)\n",
        "        [size_per_layer]   None or  with for each layer number of units (1st element = number of inputs)\n",
        "                                --> overwrites [input_size], [output_size], [layers], [hid_size] and [hid_smooth]\n",
        "        [nl]               ; type of non-linearity to be used (options: \"relu\", \"leakyrelu\", \"none\")\n",
        "        [output]           ; if - \"normal\", final layer is same as all others\n",
        "                                     - \"none\", final layer has no non-linearity\n",
        "                                     - \"sigmoid\", final layer has sigmoid non-linearity'''\n",
        "\n",
        "        super().__init__()\n",
        "        self.output = output\n",
        "\n",
        "        # get sizes of all layers\n",
        "        if size_per_layer is None:\n",
        "            hidden_sizes = []\n",
        "            if layers > 1:\n",
        "                if (hid_smooth is not None):\n",
        "                    hidden_sizes = [int(x) for x in np.linspace(hid_size, hid_smooth, num=layers-1)]\n",
        "                else:\n",
        "                    hidden_sizes = [int(x) for x in np.repeat(hid_size, layers - 1)]\n",
        "            size_per_layer = [input_size] + hidden_sizes + [output_size] if layers>0 else [input_size]\n",
        "        self.layers = len(size_per_layer)-1\n",
        "\n",
        "        # set label for this module\n",
        "        # -determine \"non-default options\"-label\n",
        "        nd_label = \"{bias}{nl}\".format(\n",
        "            bias=\"\" if bias else \"n\",\n",
        "            nl=\"l\" if nl==\"leakyrelu\" else (\"n\" if nl==\"none\" else \"\"),\n",
        "        )\n",
        "        nd_label = \"{}{}\".format(\"\" if nd_label==\"\" else \"-{}\".format(nd_label),\n",
        "                                 \"\" if output==\"normal\" else \"-{}\".format(output))\n",
        "        # -set label\n",
        "        size_statement = \"\"\n",
        "        for i in size_per_layer:\n",
        "            size_statement += \"{}{}\".format(\"-\" if size_statement==\"\" else \"x\", i)\n",
        "        self.label = \"F{}{}\".format(size_statement, nd_label) if self.layers>0 else \"\"\n",
        "\n",
        "        # set layers\n",
        "        for lay_id in range(1, self.layers+1):\n",
        "            # number of units of this layer's input and output\n",
        "            in_size = size_per_layer[lay_id-1]\n",
        "            out_size = size_per_layer[lay_id]\n",
        "            # define and set the fully connected layer\n",
        "            layer = fc_layer(\n",
        "                in_size, out_size, bias=bias,\n",
        "                nl=(\"none\" if output==\"none\" else nn.Sigmoid()) if (\n",
        "                    lay_id==self.layers and not output==\"normal\"\n",
        "                ) else nl,\n",
        "            )\n",
        "            setattr(self, 'fcLayer{}'.format(lay_id), layer)\n",
        "\n",
        "        # if no layers, add \"identity\"-module to indicate in this module's representation nothing happens\n",
        "        if self.layers<1:\n",
        "            self.noLayers = Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        for lay_id in range(1, self.layers + 1):\n",
        "            x = getattr(self, \"fcLayer{}\".format(lay_id))(x)\n",
        "        return x\n",
        "\n",
        "    @property\n",
        "    def name(self):\n",
        "        return self.label\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Classifier(torch.nn.Module):\n",
        "    '''Model for classifying images.'''\n",
        "\n",
        "    def __init__(self, image_size, image_channels, output_units,\n",
        "                 fc_layers=3, fc_units=1000, fc_nl=\"relu\", bias=True):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "       # Flatten image to 2D-tensor\n",
        "        self.flatten = Flatten()\n",
        "\n",
        "        # Specify the fully connected hidden layers\n",
        "        input_size = image_channels * image_size * image_size\n",
        "        self.fcE = MLP(input_size=input_size, output_size=fc_units, layers=fc_layers-1,\n",
        "                       hid_size=fc_units, nl=fc_nl, bias=bias)\n",
        "        mlp_output_size = fc_units if fc_layers>1 else self.input_size\n",
        "\n",
        "        # Specify the final linear classifier layer\n",
        "        self.classifier = fc_layer(mlp_output_size, output_units, nl='none')\n",
        "\n",
        "    def forward(self, x):\n",
        "        flatten_x = self.flatten(x)\n",
        "        final_features = self.fcE(flatten_x)\n",
        "        out = self.classifier(final_features)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "N9HRb_fkGCQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Specify the architectural layout of the network to use\n",
        "fc_lay = 4        #--> number of fully-connected layers\n",
        "fc_units = 40     #--> number of units in each hidden layer\n",
        "fc_nl = \"relu\"    #--> what non-linearity to use?"
      ],
      "metadata": {
        "id": "c-H632YVGiW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = Classifier(image_size=config['size'], image_channels=config['channels'],\n",
        "                   output_units=config['classes'],\n",
        "                   fc_layers=fc_lay, fc_units=fc_units, fc_nl=fc_nl)"
      ],
      "metadata": {
        "id": "Tgl9grUaG-lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "nK2iftApHB48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = 0\n",
        "for param in model.parameters():\n",
        "    n_params = index_dims = 0\n",
        "    for dim in param.size():\n",
        "        n_params = dim if index_dims==0 else n_params*dim\n",
        "        index_dims += 1\n",
        "    total_params += n_params\n",
        "print( \"--> this network has {} parameters (~{}K)\"\n",
        "      .format(total_params, round(total_params / 1000)))"
      ],
      "metadata": {
        "id": "sD8sCZk8HFnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataset, iters, lr, batch_size):\n",
        "    # Define the optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))\n",
        "\n",
        "    # Set model in training-mode\n",
        "    model.train()\n",
        "\n",
        "    # Initialize # iters left on current data-loader(s)\n",
        "    iters_left = 1\n",
        "\n",
        "    # Define tqdm progress bar(s)\n",
        "    progress_bar = tqdm.tqdm(range(1, iters+1))\n",
        "\n",
        "    # Loop over all iterations\n",
        "    for batch_index in range(1, iters+1):\n",
        "\n",
        "        # Update # iters left on current data-loader(s) and, if needed, create new one(s)\n",
        "        iters_left -= 1\n",
        "        if iters_left==0:\n",
        "            data_loader = iter(torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                                          shuffle=True, drop_last=True))\n",
        "            iters_left = len(data_loader)\n",
        "\n",
        "        # Sample training data of current context\n",
        "        x, y = next(data_loader)\n",
        "\n",
        "        # Reset optimizer\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Run model\n",
        "        y_hat = model(x)\n",
        "\n",
        "        # Calculate prediction loss\n",
        "        loss = torch.nn.functional.cross_entropy(input=y_hat, target=y, reduction='mean')\n",
        "\n",
        "        # Calculate training-accuracy (in %)\n",
        "        accuracy = (y == y_hat.max(1)[1]).sum().item()*100 / x.size(0)\n",
        "\n",
        "        # Backpropagate errors\n",
        "        loss.backward()\n",
        "\n",
        "        # Take the optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update progress bar\n",
        "        progress_bar.set_description(\n",
        "        ' | training loss: {loss:.3} | training accuracy: {prec:.3}% |'\n",
        "            .format(loss=loss.item(), prec=accuracy)\n",
        "        )\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    # Close the progress bar\n",
        "    progress_bar.close()"
      ],
      "metadata": {
        "id": "TdXV4VX3HHbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iters = 200       # for how many iterations to train?\n",
        "lr = 0.01         # learning rate\n",
        "batch_size = 128  # size of mini-batches"
      ],
      "metadata": {
        "id": "6_0jhq6AHQJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, dataset=train_datasets[0], iters=iters, lr=lr, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "0Qevfkw0HUrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def test_acc(model, dataset, test_size=None, batch_size=128):\n",
        "    '''Evaluate accuracy (% samples classified correctly) of a classifier ([model]) on [dataset].'''\n",
        "\n",
        "    # Set model to eval()-mode\n",
        "    mode = model.training\n",
        "    model.eval()\n",
        "\n",
        "    # Loop over batches in [dataset]\n",
        "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                              shuffle=True, drop_last=False)\n",
        "    total_tested = total_correct = 0\n",
        "    for x, y in data_loader:\n",
        "        # -break on [test_size] (if \"None\", full dataset is used)\n",
        "        if test_size:\n",
        "            if total_tested >= test_size:\n",
        "                break\n",
        "        # -evaluate model\n",
        "        with torch.no_grad():\n",
        "            scores = model(x)\n",
        "        _, predicted = torch.max(scores, 1)\n",
        "        # -update statistics\n",
        "        total_correct += (predicted == y).sum().item()\n",
        "        total_tested += len(x)\n",
        "    accuracy = total_correct*100 / total_tested\n",
        "\n",
        "    # Set model back to its initial mode, print result on screen (if requested) and return it\n",
        "    model.train(mode=mode)\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "poavnRjtHYhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Evaluate accuracy per context and print to screen\n",
        "print(\"\\n Accuracy (in %) of the model on test-set of:\")\n",
        "context1_accs = []\n",
        "for i in range(contexts):\n",
        "    acc = test_acc(model, test_datasets[i], test_size=None)\n",
        "    print(\" - Context {}: {:.1f}\".format(i+1, acc))\n",
        "    context1_accs.append(acc)"
      ],
      "metadata": {
        "id": "7HyZU3Y7HkBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_after_context1 = copy.deepcopy(model)"
      ],
      "metadata": {
        "id": "1Ce2VDPTHnYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Continue to train the model on the second context\n",
        "train(model, dataset=train_datasets[1], iters=iters, lr=lr, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "0empuEKZH6JR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate accuracy per context and print to screen\n",
        "print(\"\\n Accuracy (in %) of the model on test-set of:\")\n",
        "context2_accs = []\n",
        "for i in range(contexts):\n",
        "    acc = test_acc(model, test_datasets[i], test_size=None)\n",
        "    print(\" - Context {}: {:.1f}\".format(i+1, acc))\n",
        "    context2_accs.append(acc)"
      ],
      "metadata": {
        "id": "IwLP0tSVH8qL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a new model with same architecture\n",
        "model_joint = Classifier(image_size=config['size'], image_channels=config['channels'],\n",
        "                         output_units=config['classes'],\n",
        "                         fc_layers=fc_lay, fc_units=fc_units, fc_nl=fc_nl)"
      ],
      "metadata": {
        "id": "Q5RUcdu1IA_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a joint dataset with data from both contexts\n",
        "joint_trainset = torch.utils.data.ConcatDataset(train_datasets)"
      ],
      "metadata": {
        "id": "j9-6t0uvIE-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size_joint = 256\n"
      ],
      "metadata": {
        "id": "vjlnq226IHtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the joint model\n",
        "train(model_joint, dataset=joint_trainset, iters=iters, lr=lr, batch_size=batch_size_joint)"
      ],
      "metadata": {
        "id": "7-ytgu6DIJoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Evaluate the model\n",
        "print(\"\\n Accuracy (in %) of the model on test-set of:\")\n",
        "joint_accs = []\n",
        "for i in range(contexts):\n",
        "    acc = test_acc(model_joint, test_datasets[i], test_size=None)\n",
        "    print(\" - Context {}: {:.1f}\".format(i+1, acc))\n",
        "    joint_accs.append(acc)\n"
      ],
      "metadata": {
        "id": "ZcnVmz7SIMQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Visualize\n",
        "figure, axis = plt.subplots(1, 4, figsize=(15, 5))\n",
        "\n",
        "title='After training on context 1, \\nbut not yet training on context 2'\n",
        "multi_context_barplot(axis[0], context1_accs, title)\n",
        "\n",
        "title='After first training on context 1, \\nand then training on context 2'\n",
        "multi_context_barplot(axis[1], context2_accs, title)\n",
        "\n",
        "axis[2].axis('off')\n",
        "\n",
        "title='After jointly training on both contexts'\n",
        "multi_context_barplot(axis[3], joint_accs, title)"
      ],
      "metadata": {
        "id": "Qfdgt4P_IRHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ewc = copy.deepcopy(model_after_context1)\n",
        "model_replay = copy.deepcopy(model_after_context1)"
      ],
      "metadata": {
        "id": "sP51Jr1VIUPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_fisher(model, dataset, n_samples, ewc_gamma=1.):\n",
        "    '''Estimate diagonal of Fisher Information matrix for [model] on [dataset] using [n_samples].'''\n",
        "\n",
        "    # Prepare <dict> to store estimated Fisher Information matrix\n",
        "    est_fisher_info = {}\n",
        "    for n, p in model.named_parameters():\n",
        "        n = n.replace('.', '__')\n",
        "        est_fisher_info[n] = p.detach().clone().zero_()\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    mode = model.training\n",
        "    model.eval()\n",
        "\n",
        "    # Create data-loader to give batches of size 1\n",
        "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=1)\n",
        "\n",
        "    # Estimate the FI-matrix for [n_samples] batches of size 1\n",
        "    for index,(x,y) in enumerate(data_loader):\n",
        "        # break from for-loop if max number of samples has been reached\n",
        "        if n_samples is not None:\n",
        "            if index > n_samples:\n",
        "                break\n",
        "        # run forward pass of model\n",
        "        output = model(x)\n",
        "        # calculate the FI-matrix\n",
        "        with torch.no_grad():\n",
        "            label_weights = F.softmax(output, dim=1)  #--> get weights, with no gradient tracked\n",
        "        # - loop over all classes\n",
        "        for label_index in range(output.shape[1]):\n",
        "            label = torch.LongTensor([label_index])\n",
        "            negloglikelihood = F.cross_entropy(output, label)\n",
        "            # Calculate gradient of negative loglikelihood for this class\n",
        "            model.zero_grad()\n",
        "            negloglikelihood.backward(retain_graph=True if (label_index+1)<output.shape[1] else False)\n",
        "            # Square gradients and keep running sum (using the weights)\n",
        "            for n, p in model.named_parameters():\n",
        "                n = n.replace('.', '__')\n",
        "                if p.grad is not None:\n",
        "                    est_fisher_info[n] += label_weights[0][label_index] * (p.grad.detach() ** 2)\n",
        "\n",
        "    # Normalize by sample size used for estimation\n",
        "    est_fisher_info = {n: p/index for n, p in est_fisher_info.items()}\n",
        "\n",
        "    # Store new values in the network\n",
        "    for n, p in model.named_parameters():\n",
        "        n = n.replace('.', '__')\n",
        "        # -mode (=MAP parameter estimate)\n",
        "        model.register_buffer('{}_EWC_param_values'.format(n,), p.detach().clone())\n",
        "        # -precision (approximated by diagonal Fisher Information matrix)\n",
        "        if hasattr(model, '{}_EWC_estimated_fisher'.format(n)):\n",
        "            existing_values = getattr(model, '{}_EWC_estimated_fisher'.format(n))\n",
        "            est_fisher_info[n] += ewc_gamma * existing_values\n",
        "        model.register_buffer('{}_EWC_estimated_fisher'.format(n), est_fisher_info[n])\n",
        "\n",
        "    # Set model back to its initial mode\n",
        "    model.train(mode=mode)"
      ],
      "metadata": {
        "id": "zDU2VwSFORhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (only the steps that differ from the original `train`-function are commented)\n",
        "def train_ewc(model, dataset, iters, lr, batch_size, current_context, ewc_lambda=100.):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))\n",
        "    model.train()\n",
        "    iters_left = 1\n",
        "    progress_bar = tqdm.tqdm(range(1, iters+1))\n",
        "\n",
        "    for batch_index in range(1, iters+1):\n",
        "        iters_left -= 1\n",
        "        if iters_left==0:\n",
        "            data_loader = iter(torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                                           shuffle=True, drop_last=True))\n",
        "            iters_left = len(data_loader)\n",
        "        x, y = next(data_loader)\n",
        "        optimizer.zero_grad()\n",
        "        y_hat = model(x)\n",
        "        loss = torch.nn.functional.cross_entropy(input=y_hat, target=y, reduction='mean')\n",
        "\n",
        "        # Compute the EWC-regularization term, and add it to the loss (except if first context)\n",
        "        if current_context>1:\n",
        "            ewc_losses = []\n",
        "            for n, p in model.named_parameters():\n",
        "                # Retrieve stored mode (MAP estimate) and precision (Fisher Information matrix)\n",
        "                n = n.replace('.', '__')\n",
        "                mean = getattr(model, '{}_EWC_param_values'.format(n))\n",
        "                fisher = getattr(model, '{}_EWC_estimated_fisher'.format(n))\n",
        "                # Calculate weight regularization loss\n",
        "                ewc_losses.append((fisher * (p-mean)**2).sum())\n",
        "            ewc_loss = (1./2)*sum(ewc_losses)\n",
        "            total_loss = loss + ewc_lambda*ewc_loss\n",
        "        else:\n",
        "            total_loss = loss\n",
        "\n",
        "        accuracy = (y == y_hat.max(1)[1]).sum().item()*100 / x.size(0)\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "        progress_bar.set_description(\n",
        "        '<CLASSIFIER> | training loss: {loss:.3} | training accuracy: {prec:.3}% |'\n",
        "            .format(loss=total_loss.item(), prec=accuracy)\n",
        "        )\n",
        "        progress_bar.update(1)\n",
        "    progress_bar.close()"
      ],
      "metadata": {
        "id": "p68cRQhkOVCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "estimate_fisher(model_ewc, train_datasets[0], n_samples=200)"
      ],
      "metadata": {
        "id": "tFr0i4JJOYvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train on the second context using EWC parameter regularization\n",
        "ewc_lambda = 100   #--> this is a \"continual learning hyperparameter\", setting these is a delicate\n",
        "                   #    business. Here we ignore that and just use one that gives good performance.\n",
        "train_ewc(model_ewc, train_datasets[1], iters=iters, lr=lr, batch_size=batch_size,\n",
        "          current_context=2, ewc_lambda=ewc_lambda)"
      ],
      "metadata": {
        "id": "vqxGWJzIObFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "print(\"\\n Accuracy (in %) of the model on test-set of:\")\n",
        "ewc_accs = []\n",
        "for i in range(contexts):\n",
        "    acc = test_acc(model_ewc, test_datasets[i], test_size=None)\n",
        "    print(\" - Context {}: {:.1f}\".format(i+1, acc))\n",
        "    ewc_accs.append(acc)"
      ],
      "metadata": {
        "id": "cQIYxTeGOeuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper dataset classes for constructing memory buffer\n",
        "class SubDataset(torch.utils.data.Dataset):\n",
        "    '''To sub-sample a dataset, taking only those samples with label in [sub_labels].\n",
        "\n",
        "    After this selection of samples has been made, it is possible to transform the target-labels,\n",
        "    which can be useful when doing continual learning with fixed number of output units.'''\n",
        "\n",
        "    def __init__(self, original_dataset, sub_labels, target_transform=None):\n",
        "        super().__init__()\n",
        "        self.dataset = original_dataset\n",
        "        self.sub_indeces = []\n",
        "        for index in range(len(self.dataset)):\n",
        "            if hasattr(original_dataset, \"targets\"):\n",
        "                if self.dataset.target_transform is None:\n",
        "                    label = self.dataset.targets[index]\n",
        "                else:\n",
        "                    label = self.dataset.target_transform(self.dataset.targets[index])\n",
        "            else:\n",
        "                label = self.dataset[index][1]\n",
        "            if label in sub_labels:\n",
        "                self.sub_indeces.append(index)\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sub_indeces)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sample = self.dataset[self.sub_indeces[index]]\n",
        "        if self.target_transform:\n",
        "            target = self.target_transform(sample[1])\n",
        "            sample = (sample[0], target)\n",
        "        return sample\n",
        "\n",
        "\n",
        "class MemorySetDataset(torch.utils.data.Dataset):\n",
        "    '''Create dataset from list of <np.arrays> with shape (N, C, H, W) (i.e., with N images each).\n",
        "\n",
        "    The images at the i-th entry of [memory_sets] belong to class [i],\n",
        "    unless a [target_transform] is specified\n",
        "    '''\n",
        "\n",
        "    def __init__(self, memory_sets, target_transform=None):\n",
        "        super().__init__()\n",
        "        self.memory_sets = memory_sets\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        total = 0\n",
        "        for class_id in range(len(self.memory_sets)):\n",
        "            total += len(self.memory_sets[class_id])\n",
        "        return total\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        total = 0\n",
        "        for class_id in range(len(self.memory_sets)):\n",
        "            examples_in_this_class = len(self.memory_sets[class_id])\n",
        "            if index < (total + examples_in_this_class):\n",
        "                class_id_to_return = class_id if self.target_transform is None else self.target_transform(class_id)\n",
        "                example_id = index - total\n",
        "                break\n",
        "            else:\n",
        "                total += examples_in_this_class\n",
        "        image = torch.from_numpy(self.memory_sets[class_id][example_id])\n",
        "        return (image, class_id_to_return)"
      ],
      "metadata": {
        "id": "0QAioxu1O9CC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill the memory buffer using class-balanced random sampling\n",
        "def fill_memory_buffer(memory_sets, dataset, buffer_size_per_class, class_indeces):\n",
        "    '''This function is rather slow and can be optimized.'''\n",
        "    for class_id in class_indeces:\n",
        "        # Create dataset with only instances of one class\n",
        "        class_dataset = SubDataset(original_dataset=dataset, sub_labels=[class_id])\n",
        "\n",
        "        # Randomly select which indeces to store in the buffer\n",
        "        n_total = len(class_dataset)\n",
        "        indeces_selected = np.random.choice(n_total, size=min(buffer_size_per_class, n_total),\n",
        "                                            replace=False)\n",
        "\n",
        "        # Select those indeces\n",
        "        memory_set = []\n",
        "        for k in indeces_selected:\n",
        "            memory_set.append(class_dataset[k][0].numpy())\n",
        "\n",
        "        # Add this [memory_set] as a [n]x[ich]x[isz]x[isz] to the list of [memory_sets]\n",
        "        memory_sets.append(np.array(memory_set))\n",
        "\n",
        "    return memory_sets"
      ],
      "metadata": {
        "id": "KHmYKzyvOknu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "buffer_size_per_class = 20\n",
        "memory_sets = []\n",
        "# The next command is unneccesary slow, apologies! Bonus question: optimize this implementation :)\n",
        "memory_sets = fill_memory_buffer(memory_sets, train_datasets[0],\n",
        "                                 buffer_size_per_class=buffer_size_per_class,\n",
        "                                 class_indeces=list(range(10)))\n",
        "buffer_dataset = MemorySetDataset(memory_sets)"
      ],
      "metadata": {
        "id": "xgu63HD-OoFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GfXwyKYKOqWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (only the steps that differ from the original `train`-function are commented)\n",
        "def train_replay(model, dataset, iters, lr, batch_size, current_context, buffer_dataset=None):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))\n",
        "    model.train()\n",
        "    iters_left = 1\n",
        "    iters_left_replay = 1\n",
        "    progress_bar = tqdm.tqdm(range(1, iters+1))\n",
        "\n",
        "    for batch_index in range(1, iters+1):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Data from current context\n",
        "        iters_left -= 1\n",
        "        if iters_left==0:\n",
        "            data_loader = iter(torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                                           shuffle=True, drop_last=True))\n",
        "            iters_left = len(data_loader)\n",
        "        x, y = next(data_loader)\n",
        "        y_hat = model(x)\n",
        "        loss = torch.nn.functional.cross_entropy(input=y_hat, target=y, reduction='mean')\n",
        "        accuracy = (y == y_hat.max(1)[1]).sum().item()*100 / x.size(0)\n",
        "\n",
        "        # Replay data from memory buffer\n",
        "        if buffer_dataset is not None:\n",
        "          iters_left_replay -= 1\n",
        "          if iters_left_replay==0:\n",
        "              batch_size_to_use = min(batch_size, len(buffer_dataset))\n",
        "              data_loader_replay = iter(torch.utils.data.DataLoader(buffer_dataset,\n",
        "                                                                    batch_size_to_use, shuffle=True,\n",
        "                                                                    drop_last=True))\n",
        "              iters_left_replay = len(data_loader_replay)\n",
        "          x_, y_ = next(data_loader_replay)\n",
        "          y_hat_ = model(x_)\n",
        "          loss_replay = torch.nn.functional.cross_entropy(input=y_hat_, target=y_, reduction='mean')\n",
        "\n",
        "        # Combine both losses to approximate the joint loss over both contexts\n",
        "        # (i.e., the loss on the replayed data has weight proportional to number of contexts so far)\n",
        "        if buffer_dataset is not None:\n",
        "            rnt = 1./current_context\n",
        "            total_loss = rnt*loss + (1-rnt)*loss_replay\n",
        "        else:\n",
        "            total_loss = loss\n",
        "\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "        progress_bar.set_description(\n",
        "        '<CLASSIFIER> | training loss: {loss:.3} | training accuracy: {prec:.3}% |'\n",
        "            .format(loss=total_loss.item(), prec=accuracy)\n",
        "        )\n",
        "        progress_bar.update(1)\n",
        "    progress_bar.close()"
      ],
      "metadata": {
        "id": "FVfyNqsMPIf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train on the second context using experience replay\n",
        "train_replay(model_replay, train_datasets[1], iters=iters, lr=lr, batch_size=batch_size,\n",
        "             current_context=2, buffer_dataset=buffer_dataset)"
      ],
      "metadata": {
        "id": "Xv7Toz0GPQUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "print(\"\\n Accuracy (in %) of the model on test-set of:\")\n",
        "replay_accs = []\n",
        "for i in range(contexts):\n",
        "    acc = test_acc(model_replay, test_datasets[i], test_size=None)\n",
        "    print(\" - Context {}: {:.1f}\".format(i+1, acc))\n",
        "    replay_accs.append(acc)"
      ],
      "metadata": {
        "id": "K9r1_r52PTNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xfigure, axis = plt.subplots(1, 3, figsize=(12, 4))\n",
        "\n",
        "title='Fine-tuning'\n",
        "multi_context_barplot(axis[0], context2_accs, title)\n",
        "\n",
        "title='EWC \\n(lambda: {})'.format(ewc_lambda)\n",
        "multi_context_barplot(axis[1], ewc_accs, title)\n",
        "\n",
        "title='Replay \\n(buffer: {} samples per class)'.format(buffer_size_per_class)\n",
        "multi_context_barplot(axis[2], replay_accs, title)"
      ],
      "metadata": {
        "id": "0PfnTno0PV6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k0APDLTdPcl3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}